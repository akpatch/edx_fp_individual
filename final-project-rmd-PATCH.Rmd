---
title: "African and Asian Conflict Prediction: Final Individual Choice Project"
author: "Allison Patch"
date: "6/17/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Perhaps one of the biggest hurdles to conflict management processes is not knowing how destructive conflicts are likely to be to human life. This project explores the applicability of machine learning techniques to understanding the likelihood of accurate contflict fatality rate prediction using data from Asia and Africa found in The Armed Conflict Location & Event Data Project (ACLED). Using the system of parameter building outlined in the EdX Machine Learning course, through this project I will explore the particular variables included in the ACLED dataset that produce the lowest Root Mean Squared Error (RMSE) for my model. RMSE is important because it is a measurement of the average error our algorithm produces, which is helpful because it gives us an idea of how reliable our algorithm is. 

The process of algorithm selection I used in my analysis was based on Matrix factorization. I chose this method because it allowed me to evaluate the success of iterative models with new parameter vectors included in each model iteration. This allows for easy comparison between models and shows how each new parameter influences the RMSE. Additionally, I used a regulariation process to see if I could improve upon my RMSE, but this did not help to lower it.

However, despite these efforts to model a reliable prediction algorithm for conflict, I was unable to get an RMSE below about 3.33. These dismal results are likely rooted in the limited data I had available as well as the relative infrequency of conflict events more generally. The dataset I used only included about 120,000 unique events across 21 years. This limits the 


##Methods

The first step to the process was downloading the data, cleaning it, and splitting it into a training (data name: train) and test set (data name: test). For my project I adapting some code from a kaggle kernal (https://www.kaggle.com/ambarish/eda-african-conflicts-with-leaflets/code). Following this, I conducted a bit of data exploration guided by my expectations of variables that would be important to predicting the number of fatalities. Finally, I developed several iterative models using the training set and measuring the quality of the model by calculating the RMSE using the test set for each model. The first two sections of code can be found together below. The remainder of the report will discuss the results of my recommendation system algorithm exploration.

```{r Data Import}
#######################################################################################################################
# Create train and test set ###########################################################################################
#######################################################################################################################

# Note: this process could take a couple of minutes


if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")


library(readr)
library(knitr)
library(caret)
library(tidyverse)

##The data used for this project can be found in my github through the following link

# https://github.com/akpatch/edx_fp_individual.git


##If you plan to run the code, please be sure to update the path for the data as it is not linked directly

# Note I used code from a kaggle kernal when cleaning the data 
acled_af_asia<-read_csv("C:/Users/APATCH/OneDrive - UMUC/Data Science Course/African Conflict/asia_africa_conflicts.csv", col_types = cols(
  .default = col_character(),
  FATALITIES = col_integer(),
  GEO_PRECISION = col_integer(),
  GWNO = col_integer(),
  INTER1 = col_integer(),
  INTER2 = col_integer(),
  INTERACTION = col_integer(),
  LATITUDE = col_character(),
  LONGITUDE = col_character(),
  TIME_PRECISION = col_integer(),
  YEAR = col_integer()
))



acled_af_asia$LATITUDE[!grepl("^[0-9.]+$", acled_af_asia$LATITUDE)] <- NA
acled_af_asia$LONGITUDE[!grepl("^[0-9.]+$", acled_af_asia$LONGITUDE)] <- NA

acled_af_asia$LATITUDE = as.numeric(as.character(acled_af_asia$LATITUDE))
acled_af_asia$LONGITUDE = as.numeric(as.character(acled_af_asia$LONGITUDE))


acled_af_asia$EVENT_TYPE = trimws(acled_af_asia$EVENT_TYPE)
acled_af_asia$EVENT_TYPE = gsub("Strategic development","Strategic Development",acled_af_asia$EVENT_TYPE)
acled_af_asia$EVENT_TYPE = gsub("Violence against civilians","Violence Against Civilians",acled_af_asia$EVENT_TYPE)
acled_af_asia$EVENT_TYPE = gsub("Battle-no change of territory","Battle-No change of territory",acled_af_asia$EVENT_TYPE)
acled_af_asia$EVENT_TYPE = gsub("Remote violence","Remote Violence",acled_af_asia$EVENT_TYPE)
acled_af_asia$EVENT_TYPE = gsub("Strategic development","Strategic Development",acled_af_asia$EVENT_TYPE)
acled_af_asia$EVENT_TYPE = gsub("Violence against civilians","Violence Against Civilians",acled_af_asia$EVENT_TYPE)
acled_af_asia$EVENT_TYPE = gsub("Battle-no change of territory","Battle-No change of territory",acled_af_asia$EVENT_TYPE)
acled_af_asia$EVENT_TYPE = gsub("Remote violence","Remote Violence",acled_af_asia$EVENT_TYPE)


###We don't need all of the columns included in the dataset, let's keep just a few of interest
acled_af_asia2<-select(acled_af_asia, ACTOR1, ACTOR2, ADMIN1, ADMIN2, COUNTRY, EVENT_DATE, EVENT_TYPE, FATALITIES, LOCATION, YEAR)
acled_af_asia2<-na.omit(acled_af_asia2)


###About how many fatalities are there across all of these events?
hist(acled_af_asia2$FATALITIES)

###We can see that there are very few events with more than 40 fatalities, so we will exclude these outliers from the data as they will likely skew our results
acled_af_asia2<- subset(acled_af_asia2, acled_af_asia2$FATALITIES<40)

hist(acled_af_asia2$FATALITIES)


###Finally we split the data into our test and training models
set.seed(1)
test_index <- createDataPartition(y = acled_af_asia2$FATALITIES, times = 1, p = 0.1, list = FALSE)
train<- acled_af_asia2[-test_index,]
test <- acled_af_asia2[test_index,]

```


```{r Data Exploration}


#######################################################################################################################
## Exploring Data #####################################################################################################
#######################################################################################################################

#getting row and column length
nrow(train)
ncol(train)


#frequency table of fatalities 
table(train$FATALITIES)

#number of unique countries
length(unique(train$COUNTRY))

#number of unique locations
length(unique(train$LOCATION))

#number of unique Primary Actors
length(unique(train$ACTOR1))

#number of unique Second Actors
length(unique(train$ACTOR2))

#number of unique event types
table(train$EVENT_TYPE)



#Country with highest fatality rates
highest<-train%>% group_by(COUNTRY)%>% summarize(count=length(FATALITIES)) %>% arrange(desc(count))
highest
```


###Methods--Algorithm Exploration

In thinking of how to create the most accurate prediction for fatality rates of future conflicts, we first have to reflect on the factors that go into the fatalty rates. First, there is a general average fatality count that conflicts have. Second, we can think of how the rate of fatalities in different countries compare to the average---are they better, worse, the same? Third, we need to account for the type of conflict event that we are discussing because some types of conflicts are likely to see more fatalities occur than others. Fourth, we need to account for the location of the conflict. Similar rates of fatalities are likely to occur in particular locations because people are conditioned to expect a similar rate of violence in confrontations. Finally, we need to account for the primary and secondary actors involved in the conflict. Particular actors will likely engage in similar levels of violence across all events that they participate in.

##Results
The following sections of this report exmaine each of these factors in turn, building to the final algortihm for fatality prediction that I produce. 

To begin, I created a fuction which would calculate the RMSE for each of the models.
```{r RMSE calc}
###Since the outcome of interest is RMSE, create a function to calculate RMSE
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

The overall average fatality count is the staring point to the analysis for my algorithms. The overall average was:
```{r mu, echo=FALSE}
##Calculate the average fatality rate
mu_hat <- mean(train$FATALITIES)
mu_hat
```
Using this overall average, the first model I ran used a naive bayes approach simply looking at the overall average fatalities given and how well that could predict what fatality rates in the test set would be.

``` {r nb model}
###calculate RMSE for the overall average
overall_average_rmse <- RMSE(test$FATALITIES, mu_hat)
overall_average_rmse


###Here I am creating a tibble to hold the results of the models to compare RMSE
rmse_results <- tibble(method = "Overall average", RMSE = overall_average_rmse)
```

``` {r table1, echo=FALSE, fig.width=3}
#View the table
rmse_results %>% knitr::kable()

```

The RMSE result for the Overall Average Model is almost 5, which is not very good. To improve this we will add another parameter to the model: Country Effects. This will account for the difference in each country's average fatality count from the overall average. 

```{r country avgs}
## b_hat_c is the average of the diffence between the fatality rate within a specific country minus the overall average fatality rate
country_avgs <- train %>% 
  group_by(COUNTRY) %>% 
  summarize(b_hat_c = mean(FATALITIES - mu_hat))
```


A histogram of this difference shows a slightly negatively skewed distribution.

``` {r avg country diff, echo=FALSE}

##Here we visualize the distribution of b_hat_c
country_avgs %>% qplot(b_hat_c, geom ="histogram", bins = 10, data = ., color = I("black"))
```
I then ran a second model including country effects.

```{r country effects}
##Now we use the validation data to check the RSME for the mode

#first we have to calculate b_hat_c for the validation set and calculate the predicted ratings for the test model
predicted_fatalities <- mu_hat + test %>% 
  left_join(country_avgs, by='COUNTRY') %>%
  .$b_hat_c

#next we test to see the RMSE 
model_country_rmse <- RMSE(predicted_fatalities, test$FATALITIES)

#finally we add this model to our RSME table 
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Country Effect Model",
                                     RMSE = model_country_rmse ))
```

``` {r country results, echo=FALSE}
#View the table
rmse_results %>% knitr::kable()
```

The RMSE result for the Country Effects is still not very low so I added another parameter to the model: Event Type Effects. The summary of average fatalities across event types shows that there is variation in the average fatality counts over the different types of conflict.
```{r event avgs}

## b_hat_et is the average fatality observed for each type of event recorded in the data
train %>% 
  group_by(EVENT_TYPE) %>% 
  summarize(b_hat_et= mean(FATALITIES)) 
```

I then ran a third model including Country Effects and Event Effects.

```{r country + event}
#first we have to create user averages for the test set so that we can make our predictions
event_avgs <- test %>% 
  left_join(country_avgs, by='COUNTRY') %>%
  group_by(EVENT_TYPE) %>%
  summarize(b_hat_et= mean(FATALITIES - mu_hat - b_hat_c))

## Then we have to calculate the predicted ratings for the test model
predicted_fatalities <- test %>% 
  left_join(country_avgs, by='COUNTRY') %>%
  left_join(event_avgs, by='EVENT_TYPE') %>%
  mutate(pred = mu_hat + b_hat_c + b_hat_et) %>%
  .$pred

#next we test to see the RMSE 
model_eventtype_rmse <- RMSE(predicted_fatalities, test$FATALITIES)

#finally we add this model to our RSME table 
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Country + Event Type Effects Model",  
                                     RMSE = model_eventtype_rmse ))
```

``` {r country event results, echo=FALSE}
#View the table
rmse_results %>% knitr::kable()
```

The RMSE result for the Country and Events Effects is still not very low so I added another parameter to the model: Location Effects. The summary of average fatalities across event types shows that there is variation in the average fatality counts over the different types of conflict.

```{r location avgs}

## b_hat_l is the average factality rate for each specific location observed in the dataset
location_avgs<-train %>% 
  group_by(LOCATION) %>% 
  summarize(b_hat_l = mean(FATALITIES))
```

A histogram of this difference shows a slightly positively skewed distribution.

``` {r avg location diff, echo=FALSE}

##Here we visualize the distribution of b_hat_c
location_avgs %>% qplot(b_hat_l, geom ="histogram", bins = 10, data = ., color = I("black"))
```

I then ran a third model including Country, Event, and Location Effects.

```{r country location event }


##Now we use the test data to check the RSME for the model
#first we have to create user averages for the test set so that we can make our predictions
location_avgs <- test %>% 
  left_join(country_avgs, by='COUNTRY') %>%
  left_join(event_avgs, by='EVENT_TYPE') %>%
  group_by(LOCATION) %>%
  summarize(b_hat_l = mean(FATALITIES - mu_hat - b_hat_c- b_hat_et))

## Then we have to calculate the predicted ratings for the test model
predicted_fatalities <- test %>% 
  left_join(country_avgs, by='COUNTRY') %>%
  left_join(event_avgs, by='EVENT_TYPE') %>%
  left_join(location_avgs, by='LOCATION') %>%
  mutate(pred = mu_hat + b_hat_c + b_hat_et+b_hat_l) %>%
  .$pred

#next we test to see the RMSE 
model_location_rmse <- RMSE(predicted_fatalities, test$FATALITIES)

#finally we add this model to our RSME table 
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Country + Event + Location Effects Model",  
                                     RMSE = model_location_rmse ))
```

``` {r country event location results, echo=FALSE}
#View the table
rmse_results %>% knitr::kable()
```


The RMSE result for the Country, Events, and Location Effects is still not very low so I added two more parameters to the model: Primary and Secondary Actor Effects. I first added the Primary Actor and then the Secondary Actor to see if there were independent effects and there appeared to be a slight difference. The summary of average fatalities across event types shows that there is variation in the average fatality counts over the different actors involved in the conflicts. Below is the model including just the primary actor followed by the model including both Actors.
```{r country event location actor1}
## b_hat_a is the average factality rate for each specific actor1 observed in the dataset
train %>% 
  group_by(ACTOR1) %>% 
  summarize(b_hat_a = mean(FATALITIES)) %>% 
  ggplot(aes(b_hat_a)) + 
  geom_histogram(bins = 30, color = "black")

##Now we use the test data to check the RSME for the model
#first we have to create user averages for the test set so that we can make our predictions
actor_avgs <- test %>% 
  left_join(country_avgs, by='COUNTRY') %>%
  left_join(event_avgs, by='EVENT_TYPE') %>%
  left_join(location_avgs, by='LOCATION') %>%
  group_by(ACTOR1) %>%
  summarize(b_hat_a = mean(FATALITIES - mu_hat - b_hat_c- b_hat_et- b_hat_l))

## Then we have to calculate the predicted ratings for the test model
predicted_fatalities <- test %>% 
  left_join(country_avgs, by='COUNTRY') %>%
  left_join(event_avgs, by='EVENT_TYPE') %>%
  left_join(location_avgs, by='LOCATION') %>%
  left_join(actor_avgs, by='ACTOR1') %>%
  mutate(pred = mu_hat + b_hat_c + b_hat_et+b_hat_l+ b_hat_a) %>%
  .$pred

#next we test to see the RMSE 
model_actor_rmse <- RMSE(predicted_fatalities, test$FATALITIES)

#finally we add this model to our RSME table 
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Country + Event + Location + Actor 1 Effects Model",  
                                     RMSE = model_actor_rmse ))


```


``` {r country event location actor1 results, echo=FALSE}
#View the table
rmse_results %>% knitr::kable()
```



```{r country event location actor1 actor2}


## b_hat_a2 is the average factality rate for each specific actor2 observed in the dataset
train %>% 
  group_by(ACTOR2) %>% 
  summarize(b_hat_a2 = mean(FATALITIES)) %>% 
  ggplot(aes(b_hat_a2)) + 
  geom_histogram(bins = 30, color = "black")

#first we have to create user averages for the test set so that we can make our predictions
actor2_avgs <- test %>% 
  left_join(country_avgs, by='COUNTRY') %>%
  left_join(event_avgs, by='EVENT_TYPE') %>%
  left_join(location_avgs, by='LOCATION') %>%
  left_join(actor_avgs, by='ACTOR1') %>%
  group_by(ACTOR2) %>%
  summarize(b_hat_a2 = mean(FATALITIES - mu_hat - b_hat_c- b_hat_et- b_hat_l-b_hat_a))

## Then we have to calculate the predicted ratings for the test model
predicted_fatalities <- test %>% 
  left_join(country_avgs, by='COUNTRY') %>%
  left_join(event_avgs, by='EVENT_TYPE') %>%
  left_join(location_avgs, by='LOCATION') %>%
  left_join(actor_avgs, by='ACTOR1') %>%
  left_join(actor2_avgs, by='ACTOR2') %>%
  mutate(pred = mu_hat + b_hat_c + b_hat_et+b_hat_l+ b_hat_a+ b_hat_a2) %>%
  .$pred

#next we test to see the RMSE 
model_actor2_rmse <- RMSE(predicted_fatalities, test$FATALITIES)

#finally we add this model to our RSME table 
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Country + Event + Location + Actor 1 & 2Effects Model",  
                                     RMSE = model_actor2_rmse ))



```


``` {r country event location actor1 actor2 results, echo=FALSE}
#View the table
rmse_results %>% knitr::kable()
```

The RMSE result for the Country, Events, Location, and Actor 1 & 2 Effects is still not very low so in lieu of adding any additional parameters, I added a regularization effect to attempt to account for differences in average fatality rates across countries allowing for a regularization of the rates for countries with very low fatality rates. To do so I first used cross validation to identify the most effective penalty term (lambda) for the regularization process.

```{r regularization lambda cross validation}


################################REGULARIZATION#####################################################################################
##In oder to ensure that states with different number of events are not causing interference in our model we can use regularization

##Here we use cross-validation to identify the lambda (regularization penalty term) that will be most useful in tuning our model

lambdas <- seq(0, 20, 0.25)

rmses <- sapply(lambdas, function(l){
  
  mu <- mean(train$FATALITIES)
  
  b_c <- train %>% 
    group_by(COUNTRY) %>%
    summarize(b_c = sum(FATALITIES - mu)/(n()+l))
  
  
  
  predicted_fatalities <- 
    test%>% 
    left_join(b_c, by = 'COUNTRY') %>%
    mutate(pred = mu + b_c) %>%
    pull(pred)
  
  return(RMSE(predicted_fatalities , test$FATALITIES))
})

##Next we plot the rmses across lambdas to help identify which will be the most useful for our model
qplot(lambdas, rmses)  
```

```{r lambda results, echo=FALSE}

##Now we want to identify the optimal lambda for our model
lambda <- lambdas[which.min(rmses)]
lambda

```

With this lambda, 16, I re-ran all of the previous models (excluding event tye as that had a very low effect) and find that regularization actually makes my model perform worse for almost all model types


```{r regularized models}


## b_hat_c is the average of the diffence between the fatality rate within a specific country minus the overall average fatality rate
country_avgs <- train %>% 
  group_by(COUNTRY) %>% 
  summarize(b_hat_c = sum(FATALITIES - mu_hat)/(n()+lambda), n_i = n()) 


##Here we visualize the distribution of b_hat_c
country_avgs %>% qplot(b_hat_c, geom ="histogram", bins = 10, data = ., color = I("black"))

##Now we use the validation data to check the RSME for the mode

#first we have to calculate b_hat_c for the validation set and calculate the predicted ratings for the test model
predicted_fatalities <- mu_hat + test %>% 
  left_join(country_avgs, by='COUNTRY') %>%
  .$b_hat_c

#next we test to see the RMSE 
model_country_rmse <- RMSE(predicted_fatalities, test$FATALITIES)

#finally we add this model to our RSME table 
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Country Effect Model",
                                     RMSE = model_country_rmse ))
#View the table
rmse_results %>% knitr::kable()




## b_hat_l is the average factality rate for each specific location observed in the dataset
train %>% 
  group_by(LOCATION) %>% 
  summarize(b_hat_l = mean(FATALITIES)) %>% 
  ggplot(aes(b_hat_l)) + 
  geom_histogram(bins = 30, color = "black")

##Now we use the test data to check the RSME for the model
#first we have to create user averages for the test set so that we can make our predictions
location_avgs <- test %>% 
  left_join(country_avgs, by='COUNTRY') %>%
  group_by(LOCATION) %>%
  summarize(b_hat_l = mean(FATALITIES - mu_hat - b_hat_c))

## Then we have to calculate the predicted ratings for the test model
predicted_fatalities <- test %>% 
  left_join(country_avgs, by='COUNTRY') %>%
  left_join(location_avgs, by='LOCATION') %>%
  mutate(pred = mu_hat + b_hat_c +b_hat_l) %>%
  .$pred

#next we test to see the RMSE 
model_location_rmse <- RMSE(predicted_fatalities, test$FATALITIES)

#finally we add this model to our RSME table 
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Country + Location Effects Model",  
                                     RMSE = model_location_rmse ))

#View the table
rmse_results %>% knitr::kable()

## b_hat_l is the average factality rate for each specific location observed in the dataset
train %>% 
  group_by(ACTOR1) %>% 
  summarize(b_hat_a = mean(FATALITIES)) %>% 
  ggplot(aes(b_hat_a)) + 
  geom_histogram(bins = 30, color = "black")

##Now we use the test data to check the RSME for the model
#first we have to create user averages for the test set so that we can make our predictions
actor_avgs <- test %>% 
  left_join(country_avgs, by='COUNTRY') %>%
  left_join(location_avgs, by='LOCATION') %>%
  group_by(ACTOR1) %>%
  summarize(b_hat_a = mean(FATALITIES - mu_hat - b_hat_c- - b_hat_l))

## Then we have to calculate the predicted ratings for the test model
predicted_fatalities <- test %>% 
  left_join(country_avgs, by='COUNTRY') %>%
  left_join(location_avgs, by='LOCATION') %>%
  left_join(actor_avgs, by='ACTOR1') %>%
  mutate(pred = mu_hat + b_hat_c +b_hat_l+ b_hat_a) %>%
  .$pred

#next we test to see the RMSE 
model_actor_rmse <- RMSE(predicted_fatalities, test$FATALITIES)

#finally we add this model to our RSME table 
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Country  Location + Actor 1 Effects Model",  
                                     RMSE = model_actor_rmse ))

#View the table
rmse_results %>% knitr::kable()

## b_hat_l is the average factality rate for each specific location observed in the dataset
train %>% 
  group_by(ACTOR2) %>% 
  summarize(b_hat_a2 = mean(FATALITIES)) %>% 
  ggplot(aes(b_hat_a2)) + 
  geom_histogram(bins = 30, color = "black")

##Now we use the test data to check the RSME for the model
#first we have to create user averages for the test set so that we can make our predictions
actor2_avgs <- test %>% 
  left_join(country_avgs, by='COUNTRY') %>%
  left_join(location_avgs, by='LOCATION') %>%
  left_join(actor_avgs, by='ACTOR1') %>%
  group_by(ACTOR2) %>%
  summarize(b_hat_a2 = mean(FATALITIES - mu_hat - b_hat_c- b_hat_l-b_hat_a))

## Then we have to calculate the predicted ratings for the test model
predicted_fatalities <- test %>% 
  left_join(country_avgs, by='COUNTRY') %>%
  left_join(location_avgs, by='LOCATION') %>%
  left_join(actor_avgs, by='ACTOR1') %>%
  left_join(actor2_avgs, by='ACTOR2') %>%
  mutate(pred = mu_hat + b_hat_c + b_hat_l+ b_hat_a+ b_hat_a2) %>%
  .$pred

#next we test to see the RMSE 
model_actor_rmse <- RMSE(predicted_fatalities, test$FATALITIES)

#finally we add this model to our RSME table 
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Country + Location + Actor 1 & 2 Effects Model",  
                                     RMSE = model_actor_rmse ))
```

```{r regularization results, echo=FALSE}

#View the table
rmse_results %>% knitr::kable()
```

With these new models, we see that using data regularization for country averages does nothing to help model reliability


##Conclusion

In attempting to create a high quality fatality prediction model for conflict in Asia and Africa, using the ACLED data covering conflict events over 21 years and 74 countries, I was unable to use machine learning to produce a reliable model for prdiction. Every iteration of the model that I attempted was well above 1. It appears that this data is particularly ill-suited for this type of prediction model. Knowing the type of event, the location, the actors invloved did not provide enough information to accuarately pinpoint the likely count of fatalities. Additional data may help to create a better understanding of these conflict predictions as this was a relatively small sample size to conduct machine learning with. Knowing more about the roots of the conflict or the occurance of conflict in a time-series test may help to shed light on this topic and give a more accurate prediction that would be reliable. 
 
